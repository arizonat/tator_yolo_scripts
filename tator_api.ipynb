{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tator\n",
    "\n",
    "from tator.util import clone_localization_list\n",
    "import os\n",
    "from os import makedirs\n",
    "from os.path import isdir, join\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HOST = \"https://tator.whoi.edu\"\n",
    "TOKEN = open(\"tator_token.txt\", \"r\").readlines()[0].strip()\n",
    "\n",
    "FISH_DETECTION_PROJECT = 2\n",
    "ANIMAL_BBOX_LOCALIZATION_TYPE = 2\n",
    "CUREE_VIDEO_TYPE = 2\n",
    "DIVER_SURVEY_TYPE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "api = tator.get_api(HOST, TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "api.get_project_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "api.get_version_list(FISH_DETECTION_PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting / deleting localization lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get localization lists by media_id\n",
    "media_ids = []\n",
    "\n",
    "localization_list = api.get_localization_list(\n",
    "    project=FISH_DETECTION_PROJECT, \n",
    "    type=ANIMAL_BBOX_LOCALIZATION_TYPE,\n",
    "    media_id=media_ids\n",
    ")\n",
    "print(len(localization_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get localization lists by version\n",
    "version_ids = [5]\n",
    "\n",
    "localization_list = api.get_localization_list(\n",
    "    project=FISH_DETECTION_PROJECT, \n",
    "    type=ANIMAL_BBOX_LOCALIZATION_TYPE,\n",
    "    version=version_ids\n",
    ")\n",
    "print(len(localization_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete localization lists\n",
    "media_ids = []\n",
    "\n",
    "ret = api.delete_localization_list(\n",
    "    project=FISH_DETECTION_PROJECT, \n",
    "    type=ANIMAL_BBOX_LOCALIZTION_TYPE,\n",
    "    media_id=media_ids\n",
    ")\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create YOLO training dataset from Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a list of media ids corresponding to diver / curee videos\n",
    "media_list = api.get_media_list(FISH_DETECTION_PROJECT)\n",
    "mediaId_by_mediaType = {DIVER_SURVEY_TYPE: [], CUREE_VIDEO_TYPE: []}\n",
    "for media in media_list:\n",
    "    mediaId_by_mediaType[media.meta].append(media.id)\n",
    "print(mediaId_by_mediaType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new YOLO-style dataset from a version\n",
    "\n",
    "#***********************\n",
    "#Modify these things\n",
    "#***********************\n",
    "version_ids = [5]\n",
    "media_type = CUREE_VIDEO_TYPE # DIVER_SURVEY_TYPE or CUREE_VIDEO_TYPE\n",
    "dataset_base_path = f\"/data_nvme/dxy/datasets\" # /media/data/warp_data\n",
    "offline_frame_dir = \"/data_nvme/dxy/biomap\" \n",
    "dry_run = False\n",
    "#***********************\n",
    "#***********************\n",
    "\n",
    "dataset_name = f\"wrs_{'curee' if media_type == CUREE_VIDEO_TYPE else 'diver'}_yolo_dataset_v{version_ids[0]}\"\n",
    "output_dir = f\"{dataset_base_path}/{dataset_name}/\"\n",
    "\n",
    "if not isdir(output_dir) and not dry_run:\n",
    "    makedirs(output_dir)\n",
    "    makedirs(join(output_dir, \"images\"), exist_ok=True)\n",
    "    makedirs(join(output_dir, \"labels\"), exist_ok=True)\n",
    "    makedirs(join(output_dir, \"groundtruth\"), exist_ok=True)\n",
    "\n",
    "if os.path.exists(offline_frame_dir):\n",
    "    print(\"You have a specified a path for offline frames that exists. Will try to load frames from here!\")\n",
    "    print(\"Note this code path has some assumptions about video/frame/folder naming\")\n",
    "    print(\"Ideally, some metadata in media would contain a pointer to where to find the frames locally\")\n",
    "    print(\"Be careful about off by one errors!\")\n",
    "    print(\"TODO: standardize on a pipeline for diver and curee videos\\n\")\n",
    "    trying_offline_frames = True\n",
    "else:\n",
    "    print(\"Offline frame path does not exist. Will load frames from tator!\\n\")\n",
    "    trying_offline_frames = False\n",
    "\n",
    "# Iterate through all verified images corresponding to the media ids in the media type we care about\n",
    "# (this is sketchy if you have multiple state types)\n",
    "state_list = api.get_state_list(FISH_DETECTION_PROJECT, version=version_ids, media_id=mediaId_by_mediaType[media_type])\n",
    "print(f\"Num verified frames for media type {media_type}: {len(state_list)} \")\n",
    "\n",
    "num_localizations = 0\n",
    "all_localizations = []\n",
    "\n",
    "for state in tqdm(state_list):\n",
    "    # get file info\n",
    "    media = api.get_media(state.media[0])\n",
    "    \n",
    "    if media.meta != media_type:\n",
    "        assert False # this shouldn't happen since we query state_list by media_id now\n",
    "                  \n",
    "    # get localizations\n",
    "    localizations = api.get_localization_list(FISH_DETECTION_PROJECT, version=version_ids, media_id=state.media, frame=state.frame)\n",
    "    \n",
    "\n",
    "    # CUREE videos will have something like warpauv_3_xavier4_2022-11-03-10-12-06_forward.mp4 as the media.name\n",
    "    bag_name = Path(media.name).stem.split('_forward')[0]\n",
    "    frame_str = str(state.frame).zfill(6) # NOTE: arbitrary amount of padding here\n",
    "    frame_path = f\"{offline_frame_dir}/{bag_name}/forward/vanilla/frame_{frame_str}.png\"\n",
    "    if os.path.exists(frame_path):\n",
    "        image = Image.open(frame_path)\n",
    "        assert image.height == media.height\n",
    "        assert image.width == media.width\n",
    "    else:        \n",
    "        if trying_offline_frames:\n",
    "            print(f\"Could not find {frame_path}; grabbing from tator instead\")\n",
    "        # get PIL image from tator\n",
    "        imgpath = api.get_frame(state.media[0], frames=[state.frame])\n",
    "        video = api.get_media(state.media[0])\n",
    "        images = tator.util.get_images(imgpath, video, width=media.width, height=media.height, num_images=1) #note: this function can only retrieve a max of 32 images at a time for some reason\n",
    "        assert(len(images) == 1) # should only be 1 image per state, otherwise somethings wrong\n",
    "        image = images[0]\n",
    "    \n",
    "    if dry_run:\n",
    "        continue\n",
    "    \n",
    "    # save localizations in YOLO format\n",
    "    cls = 0\n",
    "    yolo_cxywhs = np.array([[cls, localization.x+localization.width/2, localization.y+localization.height/2, localization.width, localization.height] for localization in localizations])\n",
    "    if len(yolo_cxywhs) > 0:\n",
    "        np.savetxt(join(output_dir, \"labels\", media.name+f\"_f{frame_str}.txt\"),yolo_cxywhs, fmt=\"%i %f %f %f %f\")\n",
    "    else:\n",
    "        np.savetxt(join(output_dir, \"labels\", media.name+f\"_f{frame_str}.txt\"),yolo_cxywhs)\n",
    "    # save image\n",
    "    image.save(join(output_dir, \"images\", media.name+f\"_f{frame_str}.png\"))\n",
    "    \n",
    "    # save groundtruth\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for cxywh in yolo_cxywhs:\n",
    "        [c,x,y,w,h] = cxywh\n",
    "        img_w, img_h = image.size\n",
    "        draw.rectangle([int((x-w/2)*img_w), int((y-h/2)*img_h), int((x+w/2)*img_w), int((y+h/2)*img_h)], outline=(255,0,0))\n",
    "    image.save(join(output_dir, \"groundtruth\", media.name+f\"_f{frame_str}.png\"))\n",
    "    \n",
    "    all_localizations.append(localizations)\n",
    "    num_localizations += len(localizations)\n",
    "\n",
    "print(\"Total localizations: \", num_localizations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = api.get_state_list(FISH_DETECTION_PROJECT, version=version_ids, media_id=[417])\n",
    "m = api.get_media_list(FISH_DETECTION_PROJECT)\n",
    "[x.meta for x in m]\n",
    "m = api.get_media(436)\n",
    "m.height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = api.get_media(417)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate train/test/val splits using yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_height = 1280\n",
    "image_width = 720\n",
    "output_dir = \"/data_nvme/dxy/datasets/wrs_curee_yolo_dataset_v5\"\n",
    "\n",
    "import sys\n",
    "curr_repo = Path(os.path.abspath(os.getcwd()))\n",
    "yolov5_dir = curr_repo / 'yolov5'\n",
    "print(f\"YOLOV5 directory: {yolov5_dir}\")\n",
    "if str(yolov5_dir) not in sys.path:\n",
    "    sys.path.append(str(yolov5_dir))\n",
    "\n",
    "from yolov5.utils.dataloaders import autosplit\n",
    "\n",
    "print(output_dir)\n",
    "autosplit(Path(output_dir) / \"images\", weights=(0.8, 0.1, 0.1))\n",
    "\n",
    "strs = [\"train\", \"test\", \"val\"]\n",
    "for split in strs:\n",
    "    with open(Path(output_dir) / f\"autosplit_{split}.txt\") as f:\n",
    "        image_files = f.readlines()\n",
    "        objects = 0\n",
    "        areas, widths, heights = [], [], []\n",
    "        num_bg = 0\n",
    "        num_images = len(image_files)\n",
    "        for img_file in image_files:\n",
    "            label_file = img_file.replace(\"images\", \"labels\").replace(\"png\", \"txt\").strip()\n",
    "            with open(Path(output_dir) / label_file) as label_f:\n",
    "                labels = label_f.readlines()\n",
    "                for label in labels:\n",
    "                    c,x,y,w,h = label.split()\n",
    "                    width = image_width * float(w)\n",
    "                    height = image_height * float(h)\n",
    "                    areas.append(width * height)\n",
    "                    widths.append(width)\n",
    "                    heights.append(height)\n",
    "                    \n",
    "                objects += len(labels)\n",
    "                num_bg += len(labels) == 0\n",
    "        print(f\"---------------------\")\n",
    "        print(f\"{num_images} total images, {num_bg} background\")\n",
    "        print(f\"{split}: {objects} objects\")\n",
    "        print(f\"average area of {np.mean(areas)}, ({np.mean(widths)} x {np.mean(heights)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting versions (layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "api.get_version_list(project=FISH_DETECTION_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete a whole version\n",
    "version_id = 4\n",
    "print(api.get_version(id=version_id))\n",
    "res = input(f\"Are you sure you want to delete version {version_id}? (y/n)\")\n",
    "if res == 'y':\n",
    "    print(f\"deleting...\")\n",
    "    api.delete_version(id=version_id)\n",
    "else:\n",
    "    print(f\"no action taken\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloning layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_params = {'project': 2, 'media_id': [437, 438]}\n",
    "dest_project = 2\n",
    "version_mapping = {2: 5}\n",
    "media_mapping = {437: 437, 438: 438}\n",
    "localization_type_mapping = {2: 2}\n",
    "created_ids = []\n",
    "generator = clone_localization_list(api, query_params, dest_project, version_mapping,\n",
    "                                    media_mapping, localization_type_mapping)\n",
    "for num_created, num_total, response, id_map in generator:\n",
    "    print(f\"Created {num_created} of {num_total} localizations...\")\n",
    "    created_ids += response.id\n",
    "print(f\"Finished creating {num_created} localizations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
